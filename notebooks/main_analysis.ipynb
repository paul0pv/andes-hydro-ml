{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK9HuZhqDBuy"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PROJECT: HYDRO-CLIMATIC MONITORING OF HIGH-ANDEAN WETLANDS\n",
    "# MASTER NOTEBOOK: PHYSICS-AWARE ML (STACKING), WAVELET ANALYSIS & VALIDATION\n",
    "# ==============================================================================\n",
    "# Author: Research Team\n",
    "# Context: Submitted to Remote Sensing (MDPI) / Special Issue on Hydrology\n",
    "# Description: This script implements a multi-sensor fusion approach (Sentinel-1/2 + ERA5)\n",
    "#              using an Ensemble Stacking architecture to estimate Surface Hydric Status (SHS).\n",
    "#              It includes Spatial Block Cross-Validation, Wavelet Spectral Analysis,\n",
    "#              and Domain Adaptation (Transfer Learning) for ungauged basins.\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. SETUP AND DEPENDENCY INSTALLATION\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\">>> 1. SETTING UP ENVIRONMENT...\")\n",
    "\n",
    "# --- A. SYSTEM CONFIGURATION & WARNINGS ---\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# Filter specific warnings for cleaner output\n",
    "# We ignore FutureWarnings (library updates) and UserWarnings (layout adjustments)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Install specific scientific libraries not pre-installed in standard Colab envs\n",
    "# shap: XAI interpretation\n",
    "# PyWavelets: Spectral analysis\n",
    "# xgboost/lightgbm: Gradient boosting backends\n",
    "# lime: Local explanation\n",
    "print(\"   -> Installing required libraries (SHAP, PyWavelets, LIME)...\")\n",
    "!pip install shap PyWavelets xgboost lightgbm lime --quiet\n",
    "\n",
    "# --- B. STANDARD DATA SCIENCE LIBRARIES ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# --- C. PHYSICS & SIGNAL PROCESSING ---\n",
    "import pywt  # Continuous Wavelet Transform\n",
    "from scipy import signal\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# --- D. MACHINE LEARNING MODELS & UTILS ---\n",
    "# Ensembles & Regressors\n",
    "from sklearn.ensemble import (RandomForestRegressor, StackingRegressor,\n",
    "                              GradientBoostingRegressor, HistGradientBoostingRegressor)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model Selection, Metrics & Preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- E. EXPLAINABLE AI (XAI) ---\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "# --- F. GOOGLE DRIVE MOUNT ---\n",
    "from google.colab import drive\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. GLOBAL CONFIGURATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Reproducibility Seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Visual Style Settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 6),\n",
    "    'figure.dpi': 150,\n",
    "    'figure.constrained_layout.use': True,\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'DejaVu Sans'],\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'legend.fontsize': 10,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "# Mount Google Drive (if Google Colab env)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. DATA PATHS CONFIGURATION\n",
    "# ------------------------------------------------------------------------------\n",
    "# NOTE: Ensure these CSV files are present in your Drive.\n",
    "# These files should be generated using the GEE script provided in the repository.\n",
    "\n",
    "# Path config\n",
    "if IN_COLAB:\n",
    "    print(\"Executing in Google Colab. Mounting Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "  # Update this path to your specific project folder\n",
    "  BASE_PATH = '/content/drive/MyDrive/andes-hydro-ml/data/'\n",
    "else:\n",
    "    print(\"Executing in local environment.\")\n",
    "    BASE_PATH = '../data/'\n",
    "\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "\n",
    "PATHS = {\n",
    "    # Primary Basin: Full Ramis (Used for initial exploration if needed)\n",
    "    'RAMIS': {\n",
    "        'S1':      BASE_PATH + 'Ramis_S1.csv',\n",
    "        'S2':      BASE_PATH + 'Ramis_S2.csv',\n",
    "        'ERA5':    BASE_PATH + 'Ramis_ERA5_SM.csv',\n",
    "        'ERA5_PR': BASE_PATH + 'Ramis_ERA5_PR.csv'\n",
    "    },\n",
    "    # Spatial Block: NORTH (TRAINING SET for Block-CV)\n",
    "    'NORTE': {\n",
    "        'S1':      BASE_PATH + 'Ramis_North_S1.csv',\n",
    "        'S2':      BASE_PATH + 'Ramis_North_S2.csv',\n",
    "        'ERA5':    BASE_PATH + 'Ramis_North_ERA5_SM.csv',\n",
    "        'ERA5_PR': BASE_PATH + 'Ramis_North_ERA5_PR.csv'\n",
    "    },\n",
    "    # Spatial Block: SOUTH (VALIDATION SET for Block-CV)\n",
    "    'SUR': {\n",
    "        'S1':      BASE_PATH + 'Ramis_South_S1.csv',\n",
    "        'S2':      BASE_PATH + 'Ramis_South_S2.csv',\n",
    "        'ERA5':    BASE_PATH + 'Ramis_South_ERA5_SM.csv',\n",
    "        'ERA5_PR': BASE_PATH + 'Ramis_South_ERA5_PR.csv'\n",
    "    },\n",
    "    # External Basin: ILAVE (TARGET for Transfer Learning)\n",
    "    'ILAVE': {\n",
    "        'S1':      BASE_PATH + 'Ilave_S1.csv',\n",
    "        'S2':      BASE_PATH + 'Ilave_S2.csv',\n",
    "        'ERA5':    BASE_PATH + 'Ilave_ERA5_SM.csv',\n",
    "        'ERA5_PR': BASE_PATH + 'Ilave_ERA5_PR.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ ENVIRONMENT SETUP COMPLETED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYi8me-Fe3Eu"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 2. FUNCTIONS: DATA PROCESSING (ETL), PHYSICS-AWARE FE & MODELING UTILS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"\n",
    "    Robust CSV loader with datetime parsing.\n",
    "    Removes system columns from GEE export to ensure clean dataframes.\n",
    "    \"\"\"\n",
    "    if path is None: return None\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        # GEE exports usually contain 'system:time_start'\n",
    "        if 'system:time_start' in df.columns:\n",
    "            df['fecha'] = pd.to_datetime(df['system:time_start'])\n",
    "        elif 'fecha' in df.columns:\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "        cols_drop = ['system:index', '.geo', 'system:time_start']\n",
    "        df = df.drop(columns=cols_drop, errors='ignore')\n",
    "\n",
    "        return df.sort_values('fecha').reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def hydrological_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Generates hydrological memory variables (API) and derivatives.\n",
    "    CRITICAL FOR PHYSICS-AWARE APPROACH:\n",
    "    incorporates system hysteresis (memory) into the feature space.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('fecha')\n",
    "\n",
    "    # API (Antecedent Precipitation Index)\n",
    "    # Adds vertical memory to the model:\n",
    "    # - 3 days: Immediate surface wetting (pulse)\n",
    "    # - 15 days: Basal saturation state (memory)\n",
    "    if 'Precipitacion_ERA5' in df.columns:\n",
    "        df['Precip_Acum_3d'] = df['Precipitacion_ERA5'].rolling(window=3, min_periods=1).sum()\n",
    "        df['Precip_Acum_15d'] = df['Precipitacion_ERA5'].rolling(window=15, min_periods=1).sum()\n",
    "\n",
    "    # Delta VV (Wetting/Drying trend)\n",
    "    # Captures the rate of change in backscatter\n",
    "    if 'VV' in df.columns:\n",
    "        df['Delta_VV'] = df['VV'].diff().fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_datasets_advanced(df_s1, df_s2, df_era5_sm, df_era5_pr=None):\n",
    "    \"\"\"\n",
    "    Continuous daily fusion with Gap-Filling strategies.\n",
    "    CORRECTION: Handles duplicate dates (orbital overlaps) by averaging.\n",
    "    \"\"\"\n",
    "    if df_s1 is None or df_era5_sm is None: return None\n",
    "\n",
    "    # --- DUPLICATE HANDLING STEP ---\n",
    "    # Sentinel-1 sometimes has overlapping orbits (Ascending/Descending) on the same day.\n",
    "    # We average them to get a single daily representative value.\n",
    "    df_s1 = df_s1.groupby('fecha').mean(numeric_only=True).reset_index()\n",
    "    df_era5_sm = df_era5_sm.groupby('fecha').mean(numeric_only=True).reset_index()\n",
    "\n",
    "    if df_s2 is not None:\n",
    "        df_s2 = df_s2.groupby('fecha').mean(numeric_only=True).reset_index()\n",
    "\n",
    "    if df_era5_pr is not None:\n",
    "        df_era5_pr = df_era5_pr.groupby('fecha').mean(numeric_only=True).reset_index()\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # 1. Master Daily Timeline (Ensures continuity even with gaps)\n",
    "    min_date = df_s1['fecha'].min()\n",
    "    max_date = df_s1['fecha'].max()\n",
    "    master_timeline = pd.date_range(start=min_date, end=max_date, freq='D', name='fecha')\n",
    "    df_master = pd.DataFrame(index=master_timeline).reset_index()\n",
    "\n",
    "    # 2. Preprocessing (Limited Interpolation)\n",
    "    # S1: Gap-filling max 7 days (Synoptic scale consistency)\n",
    "    s1_daily = df_s1.set_index('fecha').reindex(master_timeline).interpolate(method='time', limit=7)\n",
    "\n",
    "    # S2: Gap-filling max 14 days (Vegetation persistence is longer)\n",
    "    if df_s2 is not None:\n",
    "        s2_daily = df_s2.set_index('fecha').reindex(master_timeline).interpolate(method='time', limit=14)\n",
    "    else:\n",
    "        s2_daily = pd.DataFrame(index=master_timeline)\n",
    "\n",
    "    # ERA5: Continuous (Resample to ensure daily consistency)\n",
    "    era5_sm_daily = df_era5_sm.set_index('fecha').resample('D').mean().rename(columns={'volumetric_soil_water_layer_1': 'Humedad_ERA5'})\n",
    "\n",
    "    # Precipitation\n",
    "    if df_era5_pr is not None:\n",
    "        era5_pr_daily = df_era5_pr.set_index('fecha').resample('D').mean().rename(columns={'total_precipitation_hourly': 'Precipitacion_ERA5'})\n",
    "    else:\n",
    "        era5_pr_daily = None\n",
    "\n",
    "    # 3. Fusion (Merge)\n",
    "    df_final = df_master.merge(s1_daily, on='fecha', how='left') \\\n",
    "                        .merge(s2_daily, on='fecha', how='left') \\\n",
    "                        .merge(era5_sm_daily, on='fecha', how='left')\n",
    "\n",
    "    if era5_pr_daily is not None:\n",
    "        df_final = df_final.merge(era5_pr_daily, on='fecha', how='left')\n",
    "\n",
    "    # 4. Feature Engineering application (Physics-Aware vars)\n",
    "    df_final = hydrological_feature_engineering(df_final)\n",
    "\n",
    "    # 5. Final Cleaning (Only keep days with valid Radar and Moisture data)\n",
    "    # We require at least Sentinel-1 and ERA5 to proceed.\n",
    "    return df_final.dropna(subset=['VV', 'VH', 'Humedad_ERA5']).reset_index(drop=True)\n",
    "\n",
    "def process_basin(config):\n",
    "    \"\"\"Full ETL Pipeline wrapper.\"\"\"\n",
    "    return merge_datasets_advanced(\n",
    "        load_csv(config['S1']),\n",
    "        load_csv(config['S2']),\n",
    "        load_csv(config['ERA5']),\n",
    "        load_csv(config.get('ERA5_PR')) # Can be None\n",
    "    )\n",
    "\n",
    "# ==============================================================================\n",
    "# METRICS & EVALUATION UTILS\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_hydrological_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates comprehensive metrics including ubRMSE.\n",
    "    Matches the standards of Beck et al. (2021).\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Basic Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    bias = np.mean(y_pred - y_true)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # unbiased RMSE (ubRMSE)\n",
    "    ubrmse = np.sqrt(rmse**2 - bias**2)\n",
    "\n",
    "    # KGE Calculation\n",
    "    r = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    alpha = np.std(y_pred) / np.std(y_true)\n",
    "    beta = np.mean(y_pred) / np.mean(y_true)\n",
    "    kge = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "    return {'KGE': kge, 'ubRMSE': ubrmse, 'Bias': bias, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# ==============================================================================\n",
    "# EXPLAINABLE AI (XAI) & TRANSFER LEARNING VISUALIZATION\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_shap_interaction(shap_values, X, feature_names, col_x='VV', col_interaction='NDMI'):\n",
    "    \"\"\"\n",
    "    Plots the interaction effect between two features using constrained layout.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if col_x not in feature_names or col_interaction not in feature_names:\n",
    "            return\n",
    "\n",
    "        idx_x = feature_names.index(col_x)\n",
    "        idx_int = feature_names.index(col_interaction)\n",
    "\n",
    "        plt.figure(figsize=(10, 6), layout='constrained')\n",
    "        shap.dependence_plot(idx_x, shap_values, X, feature_names=feature_names,\n",
    "                             interaction_index=idx_int, show=False)\n",
    "        plt.title(f'Physical Interaction: {col_x} vs {col_interaction}')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not generate interaction plot: {e}\")\n",
    "\n",
    "def plot_local_lime_analysis(model, X_train, X_instance, feature_names):\n",
    "    \"\"\"\n",
    "    Uses LIME to explain a SINGLE specific prediction (e.g., Drought Peak).\n",
    "    \"\"\"\n",
    "    print(\"   -> Generating local LIME explanation...\")\n",
    "\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train,\n",
    "        feature_names=feature_names,\n",
    "        mode='regression',\n",
    "        verbose=False,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Explain the instance\n",
    "    exp = explainer.explain_instance(X_instance, model.predict, num_features=5)\n",
    "\n",
    "    # Show plot\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    plt.title(\"Local Feature Contribution (LIME) - Critical Event\")\n",
    "    plt.tight_layout() # LIME figure handles layout internally differently\n",
    "    plt.show()\n",
    "\n",
    "def compare_contextual_importance(shap_values_train, shap_values_drought, feature_names):\n",
    "    \"\"\"\n",
    "    Compares variable importance: Normal Conditions vs. Drought.\n",
    "    Visualizes the regime shift in the model's logic.\n",
    "    \"\"\"\n",
    "    # Calculate mean absolute SHAP values for each feature\n",
    "    imp_train = np.mean(np.abs(shap_values_train), axis=0)\n",
    "    imp_drought = np.mean(np.abs(shap_values_drought), axis=0)\n",
    "\n",
    "    df_imp = pd.DataFrame({\n",
    "        'Variable': feature_names,\n",
    "        'Importance_Normal': imp_train,\n",
    "        'Importance_Drought': imp_drought\n",
    "    }).set_index('Variable')\n",
    "\n",
    "    # Normalize to compare proportions (relative importance)\n",
    "    df_imp = df_imp / df_imp.sum()\n",
    "\n",
    "    # Plot\n",
    "    ax = df_imp.plot(kind='bar', figsize=(12, 5), color=['gray', '#d32f2f'], alpha=0.9, width=0.8)\n",
    "    plt.title('Physical Regime Shift: Variable Importance (Historical vs Drought 2023)', fontweight='bold')\n",
    "    plt.ylabel('Relative SHAP Importance')\n",
    "    plt.xlabel('Predictor')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def transfer_learning_domain_adaptation(model_source, df_target, features, target_col, n_calibration=100):\n",
    "    \"\"\"\n",
    "    Implements Transfer Learning via 'Residual Correction'.\n",
    "\n",
    "    1. Uses Ramis model (Source) to predict on Ilave (Target).\n",
    "    2. Takes a small sample (n_calibration) as 'Field Data'.\n",
    "    3. Trains a light Correction Model (Ridge) to learn the local BIAS.\n",
    "    4. Corrects final predictions.\n",
    "    \"\"\"\n",
    "    # 1. Split Calibration (small) and Test (large)\n",
    "    # Using fixed seed for reproducibility\n",
    "    df_calib = df_target.sample(n=n_calibration, random_state=42)\n",
    "    df_test = df_target.drop(df_calib.index)\n",
    "\n",
    "    X_calib = df_calib[features].values\n",
    "    y_calib = df_calib[target_col].values\n",
    "\n",
    "    X_test = df_test[features].values\n",
    "    y_test = df_test[target_col].values\n",
    "\n",
    "    print(f\"   -> Calibrating with {n_calibration} local samples (simulated field work)...\")\n",
    "\n",
    "    # 2. Base Prediction (Source Model)\n",
    "    pred_base_calib = model_source.predict(X_calib)\n",
    "    pred_base_test = model_source.predict(X_test)\n",
    "\n",
    "    # 3. Calculate Residuals (Systematic Error in Target Domain)\n",
    "    residuos_calib = y_calib - pred_base_calib\n",
    "\n",
    "    # 4. Train Correction Model (Fine-Tuning)\n",
    "    # Ridge is preferred over LinearRegression to avoid overfitting on small n\n",
    "    correction_model = Ridge(alpha=1.0)\n",
    "    correction_model.fit(X_calib, residuos_calib)\n",
    "\n",
    "    # 5. Predict correction for the rest of data\n",
    "    correccion_test = correction_model.predict(X_test)\n",
    "\n",
    "    # 6. Final Adapted Prediction\n",
    "    y_final_pred = pred_base_test + correccion_test\n",
    "\n",
    "    return y_test, pred_base_test, y_final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6waM26he7zn"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 3. FUNCTIONS: THEORETICAL PHYSICS (WCM) & DROUGHT INDICES\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "\n",
    "def water_cloud_model(X, A, B, C, D):\n",
    "    \"\"\"\n",
    "    Semi-empirical Water Cloud Model (WCM) equation.\n",
    "    Models radar backscatter (sigma0) as a sum of vegetation scattering and\n",
    "    attenuated soil moisture contribution.\n",
    "\n",
    "    Formula:\n",
    "        sigma_veg = A * V1 * cos(theta) * (1 - tau^2)\n",
    "        tau^2     = exp(-2 * B * V2 / cos(theta))\n",
    "        sigma_soil= C + D * SM\n",
    "        sigma_tot = sigma_veg + (tau^2 * sigma_soil)\n",
    "\n",
    "    Args:\n",
    "        X (tuple): (Soil Moisture array, Vegetation Index array).\n",
    "        A: Vegetation scattering parameter.\n",
    "        B: Vegetation attenuation parameter.\n",
    "        C: Soil intercept (calibration constant).\n",
    "        D: Soil moisture sensitivity slope.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Simulated Backscatter in Linear units (or dB, depending on C/D context).\n",
    "    \"\"\"\n",
    "    soil_moisture, veg_index = X\n",
    "\n",
    "    # Standard Incidence Angle for Sentinel-1 IW mode (~30-46 deg).\n",
    "    # We use a representative mean of 38 degrees converted to radians.\n",
    "    THETA_RAD = np.radians(38)\n",
    "\n",
    "    # 1. Two-way attenuation (Transmissivity squared)\n",
    "    # Models how much signal penetrates the canopy.\n",
    "    tau2 = np.exp(-2 * B * veg_index / np.cos(THETA_RAD))\n",
    "\n",
    "    # 2. Soil Contribution (Linear approximation in dB domain)\n",
    "    # Represents the backscatter from bare soil.\n",
    "    sigma_soil = C + D * soil_moisture\n",
    "\n",
    "    # 3. Vegetation Contribution (Water Cloud)\n",
    "    # Represents volume scattering from the canopy.\n",
    "    sigma_veg = A * veg_index * np.cos(THETA_RAD) * (1 - tau2)\n",
    "\n",
    "    # Total Backscatter\n",
    "    return sigma_veg + (tau2 * sigma_soil)\n",
    "\n",
    "def fit_water_cloud_model(df):\n",
    "    \"\"\"\n",
    "    Fits the WCM physical parameters (A, B, C, D) using Non-Linear Least Squares.\n",
    "    Used to validate that the ML model aligns with physical theory.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Optimal Parameters [A,B,C,D], R2 Score of the physical fit)\n",
    "    \"\"\"\n",
    "    # 1. Check for required columns\n",
    "    required_cols = ['Humedad_ERA5', 'NDVI', 'VV']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(\"⚠️ Missing columns for WCM (NDVI/VV/Humedad required). Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. STRICT CLEANING: WCM fitting is sensitive to outliers/Infs\n",
    "    # We create a temporary subset and drop ANY missing value or infinite.\n",
    "    df_wcm = df[required_cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # 3. Data sufficiency check\n",
    "    if len(df_wcm) < 50:\n",
    "        print(f\"⚠️ Insufficient clean data for WCM fitting (n={len(df_wcm)}). Need > 50.\")\n",
    "        return None, None\n",
    "\n",
    "    # Prepare Data\n",
    "    # X_data = (Soil Moisture, Vegetation)\n",
    "    X_data = (df_wcm['Humedad_ERA5'].values, df_wcm['NDVI'].values)\n",
    "    y_data = df_wcm['VV'].values\n",
    "\n",
    "    try:\n",
    "        # Initial guesses (p0) and physical bounds\n",
    "        # A, B > 0 (Vegetation physics)\n",
    "        # C in [-30, 0] (Typical dry soil backscatter in dB)\n",
    "        # D > 0 (Backscatter increases with moisture)\n",
    "        p0 = [0.1, 0.1, -20, 20]\n",
    "        bounds = ([0, 0, -35, 0], [2, 5, -5, 60])\n",
    "\n",
    "        popt, _ = curve_fit(\n",
    "            water_cloud_model,\n",
    "            X_data,\n",
    "            y_data,\n",
    "            p0=p0,\n",
    "            bounds=bounds,\n",
    "            maxfev=5000\n",
    "        )\n",
    "\n",
    "        # Calculate R2 to assess physical consistency\n",
    "        y_pred = water_cloud_model(X_data, *popt)\n",
    "        r2_phys = r2_score(y_data, y_pred)\n",
    "\n",
    "        return popt, r2_phys\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ WCM Fitting failed (Convergence error): {e}\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_spi(df, precip_col='Precipitacion_ERA5', scales=[3, 12]):\n",
    "    \"\"\"\n",
    "    Calculates the Standardized Precipitation Index (SPI) approximation.\n",
    "    Uses a Log-Normal distribution approach (proxy for Gamma) for computational efficiency.\n",
    "    Useful for contextualizing the drought severity.\n",
    "    \"\"\"\n",
    "    if precip_col not in df.columns:\n",
    "        return None\n",
    "\n",
    "    df_spi = df.copy()\n",
    "\n",
    "    # Handle zeros for Log calculation (replace with epsilon)\n",
    "    # ERA5 hourly precipitation sum might be 0.\n",
    "    precip = df_spi[precip_col].replace(0, 0.01)\n",
    "\n",
    "    for m in scales:\n",
    "        window_days = m * 30 # Approx days per month\n",
    "\n",
    "        # Rolling Sum (Accumulated Precipitation)\n",
    "        # We require at least one full window to calculate\n",
    "        acum = precip.rolling(window=window_days, min_periods=window_days).sum()\n",
    "\n",
    "        # Log-Normal Transformation (Simple SPI)\n",
    "        # Standard SPI uses Gamma, but Log-Normal is a widely accepted proxy for large N.\n",
    "        log_acum = np.log(acum)\n",
    "\n",
    "        # Standardization (Z-Score)\n",
    "        # (X - Mean) / Std\n",
    "        df_spi[f'SPI_{m}M'] = (log_acum - log_acum.mean()) / log_acum.std()\n",
    "\n",
    "    return df_spi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rcl4Ivq-e-yv"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 4. FUNCTIONS: ADVANCED MODELING (STACKING) & UNCERTAINTY QUANTIFICATION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def recursive_feature_selection(X, y, feature_names, n_features=5):\n",
    "    \"\"\"\n",
    "    Performs Recursive Feature Elimination (RFE) to select the most physical drivers.\n",
    "    Uses a Random Forest as the base estimator to capture non-linear importance.\n",
    "\n",
    "    Args:\n",
    "        X (array): Feature matrix.\n",
    "        y (array): Target variable.\n",
    "        feature_names (list): List of column names.\n",
    "        n_features (int): Number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "        list: Names of the selected features.\n",
    "    \"\"\"\n",
    "    # Base estimator for importance ranking\n",
    "    model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "\n",
    "    rfe = RFE(estimator=model, n_features_to_select=n_features)\n",
    "    rfe.fit(X, y)\n",
    "\n",
    "    # Filter selected features\n",
    "    selected = [n for n, s in zip(feature_names, rfe.support_) if s]\n",
    "    rejected = [n for n, s in zip(feature_names, rfe.support_) if not s]\n",
    "\n",
    "    print(f\"   -> RFE Selected Features ({len(selected)}): {selected}\")\n",
    "    print(f\"   -> RFE Discarded Features: {rejected}\")\n",
    "\n",
    "    return selected\n",
    "\n",
    "def train_stacking_ensemble(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains the 2-Level Stacking Ensemble architecture.\n",
    "\n",
    "    Architecture:\n",
    "    - Level 0 (Base Learners):\n",
    "        * Random Forest: For hierarchical structure.\n",
    "        * XGBoost & LightGBM: For gradient boosting performance.\n",
    "    - Level 1 (Meta-Learner):\n",
    "        * RidgeCV: Regularized regression with internal Cross-Validation\n",
    "          to optimally weight the base models (Bias Correction).\n",
    "\n",
    "    Returns:\n",
    "        StackingRegressor: Trained model ready for prediction.\n",
    "    \"\"\"\n",
    "    # Level 0: Diverse Base Learners\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=150, max_depth=7, random_state=42, n_jobs=-1)),\n",
    "        ('xgb', XGBRegressor(n_estimators=150, learning_rate=0.05, n_jobs=-1, random_state=42)),\n",
    "        ('lgbm', LGBMRegressor(n_estimators=150, learning_rate=0.05, verbose=-1, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    "\n",
    "    # Level 1: Meta-Learner with built-in Cross-Validation for Alpha selection\n",
    "    # alphas: Regularization strength candidates\n",
    "    meta_learner = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
    "\n",
    "    reg = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=5,       # Internal CV for stacking prediction generation\n",
    "        n_jobs=-1,\n",
    "        passthrough=False # Meta-learner only sees base predictions, not original features\n",
    "    )\n",
    "\n",
    "    reg.fit(X_train, y_train)\n",
    "    return reg\n",
    "\n",
    "def predict_uncertainty_intervals(X_train, y_train, X_val, confidence_level=0.90):\n",
    "    \"\"\"\n",
    "    Generates Prediction Intervals using Quantile Regression.\n",
    "\n",
    "    Uses HistGradientBoostingRegressor because it is highly efficient for\n",
    "    quantile loss optimization on medium-sized datasets.\n",
    "\n",
    "    Args:\n",
    "        confidence_level (float): Desired confidence (e.g., 0.90 for 90% PI).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (y_lower, y_upper) arrays.\n",
    "    \"\"\"\n",
    "    # Calculate quantiles (e.g., for 90% CI -> 5% and 95%)\n",
    "    alpha = 1 - confidence_level\n",
    "    q_low = alpha / 2\n",
    "    q_high = 1 - q_low\n",
    "\n",
    "    # Lower Bound Model (e.g., 5th percentile)\n",
    "    clf_lower = HistGradientBoostingRegressor(\n",
    "        loss='quantile',\n",
    "        quantile=q_low,\n",
    "        random_state=42,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    clf_lower.fit(X_train, y_train)\n",
    "    y_lower = clf_lower.predict(X_val)\n",
    "\n",
    "    # Upper Bound Model (e.g., 95th percentile)\n",
    "    clf_upper = HistGradientBoostingRegressor(\n",
    "        loss='quantile',\n",
    "        quantile=q_high,\n",
    "        random_state=42,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    clf_upper.fit(X_train, y_train)\n",
    "    y_upper = clf_upper.predict(X_val)\n",
    "\n",
    "    return y_lower, y_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b4vpQ6NfBO8"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 5. VISUALIZATION FUNCTIONS (PUBLICATION QUALITY)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pywt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# Note: These functions rely on 'calculate_hydrological_metrics' defined in Cell 2.\n",
    "\n",
    "def plot_wavelet_analysis(time_series, title=\"Wavelet Scalogram\", power_limit=None):\n",
    "    \"\"\"\n",
    "    Generates the Continuous Wavelet Transform (CWT) Scalogram.\n",
    "    Optimized for 'constrained_layout' (set in Cell 1).\n",
    "    \"\"\"\n",
    "    # 1. Preprocessing\n",
    "    # Resample to weekly mean to reduce high-frequency noise\n",
    "    ts = time_series.resample('7D').mean()\n",
    "    # Gap-filling: Interpolate small gaps, fill large ones with mean (flat signal)\n",
    "    ts_filled = ts.interpolate(method='time', limit=4).fillna(ts.mean())\n",
    "\n",
    "    # Check for Infs\n",
    "    if np.isinf(ts_filled.values).any():\n",
    "        ts_filled = ts_filled.replace([np.inf, -np.inf], ts.mean())\n",
    "\n",
    "    # Remove mean but keep variance (Amplitude matters for this analysis)\n",
    "    data = ts_filled.values - np.mean(ts_filled.values)\n",
    "\n",
    "    # 2. Wavelet Transform (Morlet)\n",
    "    scales = np.arange(1, 120)\n",
    "    coef, freqs = pywt.cwt(data, scales, 'morl', sampling_period=1)\n",
    "    power = (abs(coef)) ** 2\n",
    "    period = 1. / freqs\n",
    "\n",
    "    # Cone of Influence (COI) Mask\n",
    "    coi = np.zeros((len(scales), len(data)))\n",
    "    for i, p in enumerate(period):\n",
    "        lim = int(p * np.sqrt(2))\n",
    "        coi[i, :lim] = 1\n",
    "        coi[i, -lim:] = 1\n",
    "\n",
    "    # 3. Plotting\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    gs = GridSpec(1, 4, figure=fig)\n",
    "\n",
    "    # --- PANEL A: SCALOGRAM ---\n",
    "    ax1 = fig.add_subplot(gs[0, :3])\n",
    "\n",
    "    # Scale color to Historical Peak if provided\n",
    "    if power_limit is None:\n",
    "        vmax = np.percentile(power, 99)\n",
    "    else:\n",
    "        vmax = power_limit\n",
    "\n",
    "    levels = np.linspace(0, vmax, 60)\n",
    "\n",
    "    # Plot Contours\n",
    "    # extend='both' ensures values outside range are colored (saturated)\n",
    "    ax1.contourf(np.arange(len(data)), period, power, levels=levels, cmap='jet', extend='both')\n",
    "    # Plot COI (hatched area)\n",
    "    ax1.contourf(np.arange(len(data)), period, coi, levels=[0.5, 1.5], colors='none', hatches=['//'])\n",
    "\n",
    "    # Drought Marker (2023)\n",
    "    if isinstance(time_series.index, pd.DatetimeIndex):\n",
    "        start_date = time_series.index.min()\n",
    "        drought_start = pd.Timestamp('2023-01-01')\n",
    "        if drought_start > start_date:\n",
    "            idx_2023 = int((drought_start - start_date).days / 7)\n",
    "            if 0 <= idx_2023 < len(data):\n",
    "                ax1.axvline(idx_2023, c='white', ls='--', lw=2, alpha=0.8)\n",
    "                ax1.text(idx_2023 + 2, period.max()*0.8, \"DROUGHT PERIOD\",\n",
    "                         color='white', fontweight='bold', rotation=90, fontsize=9)\n",
    "\n",
    "    # Formatting\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_yticks([13, 26, 52, 104])\n",
    "    ax1.set_yticklabels(['Quarterly', 'Biannual', 'Annual', 'Biennial'])\n",
    "    ax1.set_title(title, fontweight='bold')\n",
    "    ax1.set_ylabel('Period (Weeks)')\n",
    "    ax1.set_xlabel('Time (Weeks)')\n",
    "\n",
    "    # --- PANEL B: GLOBAL SPECTRUM ---\n",
    "    ax2 = fig.add_subplot(gs[0, 3], sharey=ax1)\n",
    "    gws = np.mean(power, axis=1)\n",
    "    ax2.plot(gws, period, c='b')\n",
    "    ax2.fill_betweenx(period, 0, gws, color='b', alpha=0.2)\n",
    "    ax2.set_title('Global Spectrum')\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "    ax2.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_uncertainty_series(y_true, y_pred, y_low, y_up, dates, title):\n",
    "    \"\"\"\n",
    "    Plots the time series with 90% Prediction Intervals.\n",
    "    Displays comprehensive metrics (KGE, ubRMSE, Bias) in the title.\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    mets = calculate_hydrological_metrics(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # 90% Confidence Band\n",
    "    plt.fill_between(dates, y_low, y_up, color='gray', alpha=0.3, label='90% Prediction Interval')\n",
    "\n",
    "    # Series\n",
    "    plt.plot(dates, y_true, 'k-', lw=1, label='Observed (ERA5)', alpha=0.7)\n",
    "    plt.plot(dates, y_pred, 'r--', lw=1.5, label='Estimated (Stacking)')\n",
    "\n",
    "    # Title with Metrics\n",
    "    title_text = (f\"{title}\\n\"\n",
    "                  f\"KGE: {mets['KGE']:.2f} | ubRMSE: {mets['ubRMSE']:.3f} | Bias: {mets['Bias']:.3f}\")\n",
    "\n",
    "    plt.title(title_text, fontweight='bold')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Volumetric Soil Moisture ($m^3/m^3$)')\n",
    "\n",
    "    plt.legend(loc='upper right', fancybox=True, framealpha=0.9)\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "def plot_scatter_validation(y_true, y_pred, title, color='teal'):\n",
    "    \"\"\"\n",
    "    Generates a 1:1 Scatter Plot with Identity Line and R2 score.\n",
    "    \"\"\"\n",
    "    mets = calculate_hydrological_metrics(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Scatter points\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5, c=color, edgecolor='k', s=40)\n",
    "\n",
    "    # 1:1 Identity Line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='1:1 Identity')\n",
    "\n",
    "    # Annotations\n",
    "    plt.title(f\"{title}\\n$R^2={mets['R2']:.3f} | Bias={mets['Bias']:.3f}$\", fontweight='bold')\n",
    "    plt.xlabel('Observed (ERA5)')\n",
    "    plt.ylabel('Predicted (Stacking)')\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def quantify_seasonality_strength(time_series, period_range=(45, 60)):\n",
    "    \"\"\"\n",
    "    Calculates the Peak Seasonal Energy (95th percentile).\n",
    "    Quantifies the failure to reach peak recharge levels.\n",
    "    \"\"\"\n",
    "    # 1. Preprocessing\n",
    "    ts = time_series.resample('7D').mean()\n",
    "    ts_filled = ts.interpolate(method='time', limit=4).fillna(ts.mean())\n",
    "    data = signal.detrend(ts_filled.values)\n",
    "\n",
    "    # 2. Wavelet\n",
    "    scales = np.arange(1, 120)\n",
    "    coef, freqs = pywt.cwt(data, scales, 'morl', sampling_period=1)\n",
    "    power = (abs(coef)) ** 2\n",
    "    period = 1. / freqs\n",
    "\n",
    "    # 3. Extract Annual Band\n",
    "    mask_annual = (period >= period_range[0]) & (period <= period_range[1])\n",
    "    sap_annual = np.mean(power[mask_annual, :], axis=0)\n",
    "    sap_series = pd.Series(sap_annual, index=ts_filled.index)\n",
    "\n",
    "    # 4. STATISTICAL COMPARISON (PEAK POWER)\n",
    "    # We compare the 95th Percentile (The \"Recharge Peaks\")\n",
    "    baseline_series = sap_series[sap_series.index.year < 2023]\n",
    "    drought_series = sap_series[sap_series.index.year == 2023]\n",
    "\n",
    "    baseline_peak = np.percentile(baseline_series, 95)\n",
    "    drought_peak = np.percentile(drought_series, 95)\n",
    "\n",
    "    # Percent Drop in Peak Capacity\n",
    "    drop_pct = ((baseline_peak - drought_peak) / baseline_peak) * 100\n",
    "\n",
    "    print(f\"      [DEBUG] Baseline Peak (95th): {baseline_peak:.4f}\")\n",
    "    print(f\"      [DEBUG] Drought Peak (95th): {drought_peak:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'Baseline_Power': baseline_peak,\n",
    "        'Drought_Power': drought_peak,\n",
    "        'Drop_Percentage': drop_pct\n",
    "    }\n",
    "\n",
    "def plot_interannual_energy_scan(time_series, period_range=(45, 60)):\n",
    "    \"\"\"\n",
    "    Scans the entire timeline year-by-year to validate if the 2023 drop\n",
    "    is truly unique or a recurring artifact.\n",
    "    \"\"\"\n",
    "    # 1. Continuous Wavelet Transform (Full Series)\n",
    "    ts = time_series.resample('7D').mean()\n",
    "    ts_filled = ts.interpolate(method='time', limit=4).fillna(ts.mean())\n",
    "    data = signal.detrend(ts_filled.values)\n",
    "\n",
    "    scales = np.arange(1, 120)\n",
    "    coef, freqs = pywt.cwt(data, scales, 'morl', sampling_period=1)\n",
    "    power = (abs(coef)) ** 2\n",
    "    period = 1. / freqs\n",
    "\n",
    "    # 2. Extract Annual Energy Band\n",
    "    mask_annual = (period >= period_range[0]) & (period <= period_range[1])\n",
    "    sap_continuous = np.mean(power[mask_annual, :], axis=0)\n",
    "    sap_series = pd.Series(sap_continuous, index=ts_filled.index)\n",
    "\n",
    "    # 3. Group by Year (95th Percentile Peaks)\n",
    "    annual_peaks = sap_series.groupby(sap_series.index.year).quantile(0.95)\n",
    "\n",
    "    # 4. Statistical Thresholds\n",
    "    mean_energy = annual_peaks.mean()\n",
    "    std_energy = annual_peaks.std()\n",
    "    anomaly_threshold = mean_energy - (1.5 * std_energy) # 1.5 Sigma\n",
    "\n",
    "    # 5. Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    colors = ['#b0bec5' if x > anomaly_threshold else '#d32f2f' for x in annual_peaks.values]\n",
    "    bars = plt.bar(annual_peaks.index, annual_peaks.values, color=colors, edgecolor='k', alpha=0.8)\n",
    "\n",
    "    plt.axhline(mean_energy, color='k', linestyle='--', label=f'Mean Historic Energy ({mean_energy:.2f})')\n",
    "    plt.axhline(anomaly_threshold, color='r', linestyle=':', linewidth=2, label=r'Anomaly Threshold (-1.5$\\sigma$)')\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.title('Interannual Scan: Peak Hydrological Cycle Energy (2016-2024)', fontweight='bold')\n",
    "    plt.ylabel('Peak Spectral Power (95th Pct)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return annual_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxMY0_BO2ud_"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 6. MAIN WORKFLOW: MODELING, SPATIAL VALIDATION & EXPLAINABILITY\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "print(\"\\n>>> INITIALIZING MAIN WORKFLOW (SPATIAL BLOCK-CV & TRANSFER LEARNING)...\")\n",
    "\n",
    "try:\n",
    "    # --- PHASE 1: DATA LOADING & PREPROCESSING (TRAINING BLOCK) ---\n",
    "    print(\"\\n[PHASE 1] Loading Primary Training Data (Ramis North)...\")\n",
    "\n",
    "    # Note: 'PATHS' keys must match Cell 1 definition.\n",
    "    df_north = process_basin(PATHS['NORTE'])\n",
    "\n",
    "    if df_north is not None:\n",
    "        print(f\"✅ Ramis North Loaded: {len(df_north)} records.\")\n",
    "\n",
    "        # --- A. MODEL TRAINING (STACKING ENSEMBLE) ---\n",
    "        print(\"\\n>>> A. ADVANCED MODELING (STACKING ENSEMBLE)...\")\n",
    "\n",
    "        # 1. Feature Definition\n",
    "        all_feats = ['VV', 'VH', 'NDMI', 'NDVI', 'NDWI', 'Precip_Acum_3d', 'Precip_Acum_15d', 'Delta_VV']\n",
    "        avail_feats = [f for f in all_feats if f in df_north.columns]\n",
    "\n",
    "        # 2. Temporal Split (Train < 2023, Val = 2023)\n",
    "        df_north['year'] = df_north['fecha'].dt.year\n",
    "        df_north = df_north.dropna(subset=avail_feats).reset_index(drop=True)\n",
    "\n",
    "        mask_train = df_north['year'] < 2023\n",
    "        mask_val_drought = df_north['year'] == 2023\n",
    "\n",
    "        # 3. Recursive Feature Elimination (RFE)\n",
    "        # Performed strictly on Training Data to avoid data leakage\n",
    "        top_feats = recursive_feature_selection(\n",
    "            df_north.loc[mask_train, avail_feats].values,\n",
    "            df_north.loc[mask_train, 'Humedad_ERA5'].values,\n",
    "            avail_feats,\n",
    "            n_features=5\n",
    "        )\n",
    "\n",
    "        # Prepare matrices\n",
    "        X_full = df_north[top_feats].values\n",
    "        y_full = df_north['Humedad_ERA5'].values\n",
    "\n",
    "        X_train = X_full[mask_train]\n",
    "        y_train = y_full[mask_train]\n",
    "\n",
    "        # 4. Train Model\n",
    "        print(f\"   -> Training Stacking Ensemble on {len(X_train)} samples...\")\n",
    "        model = train_stacking_ensemble(X_train, y_train)\n",
    "\n",
    "        # --- B. PHYSICAL DIAGNOSTICS (WAVELETS & WCM) ---\n",
    "        print(\"\\n>>> B. PHYSICAL DIAGNOSTICS & CONSISTENCY CHECK...\")\n",
    "\n",
    "        # 1. WCM Fitting\n",
    "        popt, r2_wcm = fit_water_cloud_model(df_north)\n",
    "        if popt is not None:\n",
    "            print(f\"   -> WCM Fit (Physical Consistency): R²={r2_wcm:.3f} | Params: {popt}\")\n",
    "\n",
    "        # 2. Wavelet Analysis on MODEL PREDICTION\n",
    "        print(\"   -> Analyzing seasonality breakdown on Model Predictions...\")\n",
    "\n",
    "        # Generate predictions for the full timeline to analyze the whole series\n",
    "        y_pred_full = model.predict(X_full)\n",
    "        ts_pred = pd.Series(y_pred_full, index=df_north['fecha']).sort_index()\n",
    "\n",
    "        # 3. INTERANNUAL SCAN\n",
    "        print(\"\\n   -> Running Interannual Variability Scan (Checking for false positives)...\")\n",
    "        # This generates Figure 8 (Bar plot)\n",
    "        annual_energy = plot_interannual_energy_scan(ts_pred)\n",
    "\n",
    "        # 4. WAVELET SCALOGRAM\n",
    "        # We define \"Red\" (Max Power) based on the STRONGEST historical year.\n",
    "        ts_historic = ts_pred[ts_pred.index.year < 2023]\n",
    "\n",
    "        # Calculate power of history manually to find the limit\n",
    "        data_hist = ts_historic.resample('7D').mean().interpolate(limit=4).fillna(ts_historic.mean()).values\n",
    "        data_hist = signal.detrend(data_hist)\n",
    "        scales = np.arange(1, 120)\n",
    "        coef, _ = pywt.cwt(data_hist, scales, 'morl', sampling_period=1)\n",
    "        power_hist = (abs(coef)) ** 2\n",
    "        historical_peak_power = np.max(power_hist)\n",
    "\n",
    "        print(f\"      Historical Peak Power (Baseline): {historical_peak_power:.2f}\")\n",
    "\n",
    "        plot_wavelet_analysis(ts_pred,\n",
    "                              title=\"Spectral Power Density (Seasonality Attenuation)\",\n",
    "                              power_limit=historical_peak_power)\n",
    "\n",
    "        # 5. QUANTITATIVE SEASONALITY METRICS\n",
    "        print(\"\\n   -> Quantifying Energy Loss in the Annual Cycle (45-60 weeks)...\")\n",
    "        stats_wavelet = quantify_seasonality_strength(ts_pred)\n",
    "\n",
    "        print(f\"      STATS REPORT:\")\n",
    "        print(f\"      - Historical Energy: {stats_wavelet['Baseline_Power']:.4f}\")\n",
    "        print(f\"      - Drought 2023 Energy: {stats_wavelet['Drought_Power']:.4f}\")\n",
    "        print(f\"      - MAGNITUDE OF COLLAPSE: {stats_wavelet['Drop_Percentage']:.2f}% Reduction\")\n",
    "\n",
    "        # --- C. TEMPORAL VALIDATION: DROUGHT ANALYSIS & UNCERTAINTY ---\n",
    "        print(\"\\n>>> C. TEMPORAL VALIDATION: DROUGHT ANALYSIS & UNCERTAINTY...\")\n",
    "        print(\"   -> Calculating prediction intervals (90% CI)...\")\n",
    "\n",
    "        # 1. Prepare Plotting Data\n",
    "        df_plot = df_north.sort_values('fecha').copy()\n",
    "        dates_plot = df_plot['fecha']\n",
    "        y_true_plot = df_plot['Humedad_ERA5'].values\n",
    "        X_plot = df_plot[top_feats].values\n",
    "\n",
    "        # 2. Generate Uncertainty Intervals (Quantile Regression)\n",
    "        y_low, y_up = predict_uncertainty_intervals(X_train, y_train, X_plot, confidence_level=0.90)\n",
    "\n",
    "        # 3. Calculate Global Metrics\n",
    "        mets = calculate_hydrological_metrics(y_true_plot, y_pred_full)\n",
    "\n",
    "        # 4. Render\n",
    "        fig, ax = plt.subplots(figsize=(14, 6)) # DPI set globally in Cell 1\n",
    "\n",
    "        # Confidence Bands\n",
    "        ax.fill_between(dates_plot, y_low, y_up, color='#b0bec5', alpha=0.4,\n",
    "                        label='Uncertainty (90% PI)', zorder=1)\n",
    "\n",
    "        # Observations (ERA5)\n",
    "        ax.plot(dates_plot, y_true_plot, color='black', linestyle='-', linewidth=1.2,\n",
    "                label='Reference (ERA5-Land)', alpha=0.8, zorder=2)\n",
    "\n",
    "        # Predictions (Stacking)\n",
    "        ax.plot(dates_plot, y_pred_full, color='#d32f2f', linestyle='--', linewidth=1.5,\n",
    "                label='Prediction (Stacking)', zorder=3)\n",
    "\n",
    "        # Highlight Drought 2023\n",
    "        start_drought = pd.to_datetime('2023-01-01')\n",
    "        end_drought = pd.to_datetime('2023-12-31')\n",
    "        ax.axvspan(start_drought, end_drought, color='#fff9c4', alpha=0.5, zorder=0)\n",
    "\n",
    "        # Annotate Physical Breakdown\n",
    "        ax.annotate('2023 Water Anomaly\\n(Seasonality Attenuation)',\n",
    "                    xy=(pd.to_datetime('2023-06-15'), 0.15),\n",
    "                    xytext=(pd.to_datetime('2021-06-01'), 0.05),\n",
    "                    arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),\n",
    "                    fontsize=10, fontweight='bold', color='#bf360c',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"#bf360c\", alpha=0.9))\n",
    "\n",
    "        # Metrics Info Box\n",
    "        info_box = (f\"Figure 5. Global Metrics (2016-2024):\\n\"\n",
    "                    f\"KGE = {mets['KGE']:.2f}\\n\"\n",
    "                    f\"ubRMSE = {mets['ubRMSE']:.3f} m³/m³\\n\"\n",
    "                    f\"Bias = {mets['Bias']:.3f}\")\n",
    "\n",
    "        ax.text(0.02, 0.95, info_box, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "\n",
    "        # Styling\n",
    "        ax.set_ylabel('Volumetric Soil Moisture ($m^3/m^3$)', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time (Years)', fontsize=12)\n",
    "        ax.set_title('Temporal Dynamics of Surface Hydric Status and Response to Extreme Drought',\n",
    "                     fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        ax.set_ylim(0, 0.6)\n",
    "        ax.set_xlim(dates_plot.min(), dates_plot.max())\n",
    "        ax.legend(loc='upper right', frameon=True, fancybox=True, framealpha=0.9, fontsize=10)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # --- D. EXPLAINABILITY (XAI) ---\n",
    "        print(\"\\n>>> D. ADVANCED EXPLAINABILITY (XAI)...\")\n",
    "\n",
    "        # 1. SHAP (Surrogate Model)\n",
    "        rf_explainer = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "        rf_explainer.fit(X_train, y_train)\n",
    "        explainer = shap.TreeExplainer(rf_explainer)\n",
    "\n",
    "        # Compute SHAP values for Drought period vs History\n",
    "        X_val_drought = X_full[mask_val_drought]\n",
    "        shap_vals_drought = explainer.shap_values(X_val_drought)\n",
    "        shap_vals_train = explainer.shap_values(X_train)\n",
    "\n",
    "        # 2. Contextual Comparison\n",
    "        compare_contextual_importance(shap_vals_train, shap_vals_drought, top_feats)\n",
    "\n",
    "        # 3. LIME Day Zero Analysis\n",
    "        print(\"   -> Performing 'Day Zero' Autopsy (LIME)...\")\n",
    "        # Find the driest predicted day in 2023\n",
    "        y_pred_23 = y_pred_full[mask_val_drought]\n",
    "        idx_min = np.argmin(y_pred_23)\n",
    "\n",
    "        X_worst_day = X_val_drought[idx_min]\n",
    "        worst_date = df_north.loc[mask_val_drought, 'fecha'].iloc[idx_min].date()\n",
    "\n",
    "        print(f\"      Critical Date: {worst_date}\")\n",
    "        print(f\"      Predicted SHS: {y_pred_23[idx_min]:.3f} m3/m3\")\n",
    "\n",
    "        plot_local_lime_analysis(model, X_train, X_worst_day, top_feats)\n",
    "\n",
    "        # --- E. SPATIAL VALIDATION 1: INTRA-BASIN (SOUTH BLOCK) ---\n",
    "        print(\"\\n>>> E. SPATIAL CROSS-VALIDATION (BLOCK: RAMIS SOUTH)...\")\n",
    "        df_south = process_basin(PATHS['SUR'])\n",
    "\n",
    "        if df_south is not None:\n",
    "            # Clean NaNs in Target Domain\n",
    "            df_south = df_south.dropna(subset=top_feats).reset_index(drop=True)\n",
    "\n",
    "            if len(df_south) > 150:\n",
    "                y_test_s, y_naive_s, y_adapted_s = transfer_learning_domain_adaptation(\n",
    "                    model, df_south, top_feats, 'Humedad_ERA5', n_calibration=100\n",
    "                )\n",
    "                # Comparative Naive Transfer vs Fine-Tuned & Single Scatter for Intra-basin\n",
    "                plot_transfer_results(y_test_s, y_naive_s, y_adapted_s)\n",
    "                plot_scatter_validation(y_test_s, y_adapted_s, \"Spatial Validation: South Block (Fine-Tuned)\", color='teal')\n",
    "            else:\n",
    "                print(\"⚠️ Insufficient data in South Block.\")\n",
    "\n",
    "        # --- F. SPATIAL VALIDATION 2: INTER-BASIN (ILAVE) ---\n",
    "        print(\"\\n>>> F. REGIONAL SCALABILITY (TARGET: ILAVE BASIN)...\")\n",
    "        df_ilave = process_basin(PATHS['ILAVE'])\n",
    "\n",
    "        if df_ilave is not None:\n",
    "            # Clean NaNs in Target Domain\n",
    "            df_ilave = df_ilave.dropna(subset=top_feats).reset_index(drop=True)\n",
    "\n",
    "            if len(df_ilave) > 150:\n",
    "                y_test_i, y_naive_i, y_adapted_i = transfer_learning_domain_adaptation(\n",
    "                    model, df_ilave, top_feats, 'Humedad_ERA5', n_calibration=100\n",
    "                )\n",
    "                # Comparative Naive Transfer vs Fine-Tuned & Single Scatter for Inter-basin\n",
    "                plot_transfer_results(y_test_i, y_naive_i, y_adapted_i)\n",
    "                plot_scatter_validation(y_test_i, y_adapted_i, \"Spatial Validation: Ilave Transfer (Fine-Tuned)\", color='teal')\n",
    "            else:\n",
    "                print(\"⚠️ Insufficient data in Ilave Basin.\")\n",
    "        else:\n",
    "            print(\"⚠️ Ilave dataset not found. Skipping Regional Validation.\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ CRITICAL: Primary Training Data (Ramis North) failed to load.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ CRITICAL ERROR in Main Workflow: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17RAwjRpKHB6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
